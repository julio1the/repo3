{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "1d4e2743-cf7e-4d1a-89f0-1e267dabed2a",
      "metadata": {},
      "source": [
        "# Assignment 03\n",
        "\n",
        "Julio Vargas (Boston University)  \n",
        "September 21, 2025"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "f20566e7",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: Using incubator modules: jdk.incubator.vector\n",
            "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
            "Setting default log level to \"WARN\".\n",
            "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
            "25/09/21 23:31:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.functions import col, split, explode, regexp_replace, transform, when\n",
        "from pyspark.sql.functions import col, monotonically_increasing_id\n",
        "from pyspark.sql.types import StructType  # to/from JSON\n",
        "\n",
        "import json\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import plotly.express as px\n",
        "import plotly.io as pio\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "\n",
        "np.random.seed(30)  # set a fixed seed for reproducibility\n",
        "pio.renderers.default = \"notebook\"\n",
        "# Initialize Spark Session\n",
        "spark = SparkSession.builder.appName(\"JobPostingsAnalysis\").getOrCreate()\n",
        "# Load schema from JSON file\n",
        "with open(\"data/schema_lightcast.json\") as f:\n",
        "    schema = StructType.fromJson(json.load(f))\n",
        "\n",
        "# Load Data\n",
        "df = (spark.read\n",
        "      .option(\"header\", \"true\")\n",
        "      .option(\"inferSchema\", \"false\")\n",
        "      .schema(schema)              # saved schema\n",
        "      .option(\"multiLine\", \"true\")\n",
        "      .option(\"escape\", \"\\\"\")\n",
        "      .csv(\"data/lightcast_job_postings.csv\"))\n",
        "# Show Schema and Sample Data\n",
        "#df.printSchema()  \n",
        "#df.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "c14caffe",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "25/09/21 23:31:59 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
            "25/09/21 23:32:12 ERROR Executor: Exception in task 0.0 in stage 0.0 (TID 0)/ 1]\n",
            "java.lang.OutOfMemoryError: Java heap space\n",
            "\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:64)\n",
            "\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:363)\n",
            "\tat org.apache.spark.serializer.SerializerHelper$.$anonfun$serializeToChunkedBuffer$1(SerializerHelper.scala:40)\n",
            "\tat org.apache.spark.serializer.SerializerHelper$.$anonfun$serializeToChunkedBuffer$1$adapted(SerializerHelper.scala:40)\n",
            "\tat org.apache.spark.serializer.SerializerHelper$$$Lambda$3196/0x00007240ccf204b8.apply(Unknown Source)\n",
            "\tat org.apache.spark.util.io.ChunkedByteBufferOutputStream.allocateNewChunkIfNeeded(ChunkedByteBufferOutputStream.scala:87)\n",
            "\tat org.apache.spark.util.io.ChunkedByteBufferOutputStream.write(ChunkedByteBufferOutputStream.scala:75)\n",
            "\tat java.base/java.io.ObjectOutputStream$BlockDataOutputStream.write(ObjectOutputStream.java:1862)\n",
            "\tat java.base/java.io.ObjectOutputStream.write(ObjectOutputStream.java:714)\n",
            "\tat org.apache.spark.util.Utils$.$anonfun$writeByteBuffer$1(Utils.scala:217)\n",
            "\tat org.apache.spark.util.Utils$.$anonfun$writeByteBuffer$1$adapted(Utils.scala:217)\n",
            "\tat org.apache.spark.util.Utils$$$Lambda$3199/0x00007240ccf21fe0.apply(Unknown Source)\n",
            "\tat org.apache.spark.util.Utils$.writeByteBufferImpl(Utils.scala:195)\n",
            "\tat org.apache.spark.util.Utils$.writeByteBuffer(Utils.scala:217)\n",
            "\tat org.apache.spark.util.io.ChunkedByteBuffer.$anonfun$writeExternal$2(ChunkedByteBuffer.scala:104)\n",
            "\tat org.apache.spark.util.io.ChunkedByteBuffer.$anonfun$writeExternal$2$adapted(ChunkedByteBuffer.scala:104)\n",
            "\tat org.apache.spark.util.io.ChunkedByteBuffer$$Lambda$3198/0x00007240ccf21c10.apply(Unknown Source)\n",
            "\tat scala.collection.ArrayOps$.foreach$extension(ArrayOps.scala:1324)\n",
            "\tat org.apache.spark.util.io.ChunkedByteBuffer.writeExternal(ChunkedByteBuffer.scala:104)\n",
            "\tat org.apache.spark.scheduler.DirectTaskResult.$anonfun$writeExternal$1(TaskResult.scala:60)\n",
            "\tat org.apache.spark.scheduler.DirectTaskResult$$Lambda$3205/0x00007240ccf238a0.apply$mcV$sp(Unknown Source)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryOrIOException(SparkErrorUtils.scala:35)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryOrIOException$(SparkErrorUtils.scala:33)\n",
            "\tat org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:99)\n",
            "\tat org.apache.spark.scheduler.DirectTaskResult.writeExternal(TaskResult.scala:59)\n",
            "\tat java.base/java.io.ObjectOutputStream.writeExternalData(ObjectOutputStream.java:1465)\n",
            "\tat java.base/java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1436)\n",
            "\tat java.base/java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1181)\n",
            "\tat java.base/java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:350)\n",
            "\tat org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:47)\n",
            "\tat org.apache.spark.serializer.SerializerHelper$.serializeToChunkedBuffer(SerializerHelper.scala:42)\n",
            "25/09/21 23:32:12 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 0.0 in stage 0.0 (TID 0),5,main]\n",
            "java.lang.OutOfMemoryError: Java heap space\n",
            "\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:64)\n",
            "\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:363)\n",
            "\tat org.apache.spark.serializer.SerializerHelper$.$anonfun$serializeToChunkedBuffer$1(SerializerHelper.scala:40)\n",
            "\tat org.apache.spark.serializer.SerializerHelper$.$anonfun$serializeToChunkedBuffer$1$adapted(SerializerHelper.scala:40)\n",
            "\tat org.apache.spark.serializer.SerializerHelper$$$Lambda$3196/0x00007240ccf204b8.apply(Unknown Source)\n",
            "\tat org.apache.spark.util.io.ChunkedByteBufferOutputStream.allocateNewChunkIfNeeded(ChunkedByteBufferOutputStream.scala:87)\n",
            "\tat org.apache.spark.util.io.ChunkedByteBufferOutputStream.write(ChunkedByteBufferOutputStream.scala:75)\n",
            "\tat java.base/java.io.ObjectOutputStream$BlockDataOutputStream.write(ObjectOutputStream.java:1862)\n",
            "\tat java.base/java.io.ObjectOutputStream.write(ObjectOutputStream.java:714)\n",
            "\tat org.apache.spark.util.Utils$.$anonfun$writeByteBuffer$1(Utils.scala:217)\n",
            "\tat org.apache.spark.util.Utils$.$anonfun$writeByteBuffer$1$adapted(Utils.scala:217)\n",
            "\tat org.apache.spark.util.Utils$$$Lambda$3199/0x00007240ccf21fe0.apply(Unknown Source)\n",
            "\tat org.apache.spark.util.Utils$.writeByteBufferImpl(Utils.scala:195)\n",
            "\tat org.apache.spark.util.Utils$.writeByteBuffer(Utils.scala:217)\n",
            "\tat org.apache.spark.util.io.ChunkedByteBuffer.$anonfun$writeExternal$2(ChunkedByteBuffer.scala:104)\n",
            "\tat org.apache.spark.util.io.ChunkedByteBuffer.$anonfun$writeExternal$2$adapted(ChunkedByteBuffer.scala:104)\n",
            "\tat org.apache.spark.util.io.ChunkedByteBuffer$$Lambda$3198/0x00007240ccf21c10.apply(Unknown Source)\n",
            "\tat scala.collection.ArrayOps$.foreach$extension(ArrayOps.scala:1324)\n",
            "\tat org.apache.spark.util.io.ChunkedByteBuffer.writeExternal(ChunkedByteBuffer.scala:104)\n",
            "\tat org.apache.spark.scheduler.DirectTaskResult.$anonfun$writeExternal$1(TaskResult.scala:60)\n",
            "\tat org.apache.spark.scheduler.DirectTaskResult$$Lambda$3205/0x00007240ccf238a0.apply$mcV$sp(Unknown Source)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryOrIOException(SparkErrorUtils.scala:35)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryOrIOException$(SparkErrorUtils.scala:33)\n",
            "\tat org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:99)\n",
            "\tat org.apache.spark.scheduler.DirectTaskResult.writeExternal(TaskResult.scala:59)\n",
            "\tat java.base/java.io.ObjectOutputStream.writeExternalData(ObjectOutputStream.java:1465)\n",
            "\tat java.base/java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1436)\n",
            "\tat java.base/java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1181)\n",
            "\tat java.base/java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:350)\n",
            "\tat org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:47)\n",
            "\tat org.apache.spark.serializer.SerializerHelper$.serializeToChunkedBuffer(SerializerHelper.scala:42)\n",
            "25/09/21 23:32:12 WARN TaskSetManager: Lost task 0.0 in stage 0.0 (TID 0) (ip-172-31-28-19.ec2.internal executor driver): java.lang.OutOfMemoryError: Java heap space\n",
            "\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:64)\n",
            "\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:363)\n",
            "\tat org.apache.spark.serializer.SerializerHelper$.$anonfun$serializeToChunkedBuffer$1(SerializerHelper.scala:40)\n",
            "\tat org.apache.spark.serializer.SerializerHelper$.$anonfun$serializeToChunkedBuffer$1$adapted(SerializerHelper.scala:40)\n",
            "\tat org.apache.spark.serializer.SerializerHelper$$$Lambda$3196/0x00007240ccf204b8.apply(Unknown Source)\n",
            "\tat org.apache.spark.util.io.ChunkedByteBufferOutputStream.allocateNewChunkIfNeeded(ChunkedByteBufferOutputStream.scala:87)\n",
            "\tat org.apache.spark.util.io.ChunkedByteBufferOutputStream.write(ChunkedByteBufferOutputStream.scala:75)\n",
            "\tat java.base/java.io.ObjectOutputStream$BlockDataOutputStream.write(ObjectOutputStream.java:1862)\n",
            "\tat java.base/java.io.ObjectOutputStream.write(ObjectOutputStream.java:714)\n",
            "\tat org.apache.spark.util.Utils$.$anonfun$writeByteBuffer$1(Utils.scala:217)\n",
            "\tat org.apache.spark.util.Utils$.$anonfun$writeByteBuffer$1$adapted(Utils.scala:217)\n",
            "\tat org.apache.spark.util.Utils$$$Lambda$3199/0x00007240ccf21fe0.apply(Unknown Source)\n",
            "\tat org.apache.spark.util.Utils$.writeByteBufferImpl(Utils.scala:195)\n",
            "\tat org.apache.spark.util.Utils$.writeByteBuffer(Utils.scala:217)\n",
            "\tat org.apache.spark.util.io.ChunkedByteBuffer.$anonfun$writeExternal$2(ChunkedByteBuffer.scala:104)\n",
            "\tat org.apache.spark.util.io.ChunkedByteBuffer.$anonfun$writeExternal$2$adapted(ChunkedByteBuffer.scala:104)\n",
            "\tat org.apache.spark.util.io.ChunkedByteBuffer$$Lambda$3198/0x00007240ccf21c10.apply(Unknown Source)\n",
            "\tat scala.collection.ArrayOps$.foreach$extension(ArrayOps.scala:1324)\n",
            "\tat org.apache.spark.util.io.ChunkedByteBuffer.writeExternal(ChunkedByteBuffer.scala:104)\n",
            "\tat org.apache.spark.scheduler.DirectTaskResult.$anonfun$writeExternal$1(TaskResult.scala:60)\n",
            "\tat org.apache.spark.scheduler.DirectTaskResult$$Lambda$3205/0x00007240ccf238a0.apply$mcV$sp(Unknown Source)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryOrIOException(SparkErrorUtils.scala:35)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryOrIOException$(SparkErrorUtils.scala:33)\n",
            "\tat org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:99)\n",
            "\tat org.apache.spark.scheduler.DirectTaskResult.writeExternal(TaskResult.scala:59)\n",
            "\tat java.base/java.io.ObjectOutputStream.writeExternalData(ObjectOutputStream.java:1465)\n",
            "\tat java.base/java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1436)\n",
            "\tat java.base/java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1181)\n",
            "\tat java.base/java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:350)\n",
            "\tat org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:47)\n",
            "\tat org.apache.spark.serializer.SerializerHelper$.serializeToChunkedBuffer(SerializerHelper.scala:42)\n",
            "\n",
            "25/09/21 23:32:12 ERROR TaskSetManager: Task 0 in stage 0.0 failed 1 times; aborting job\n",
            "[Stage 0:>                                                          (0 + 0) / 1]\r"
          ]
        },
        {
          "ename": "ConnectionRefusedError",
          "evalue": "[Errno 111] Connection refused",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mPy4JJavaError\u001b[39m                             Traceback (most recent call last)",
            "    \u001b[31m[... skipping hidden 1 frame]\u001b[39m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m salary_df = df.filter(col(\u001b[33m\"\u001b[39m\u001b[33mSALARY\u001b[39m\u001b[33m\"\u001b[39m).isNotNull() & (col(\u001b[33m\"\u001b[39m\u001b[33mSALARY\u001b[39m\u001b[33m\"\u001b[39m) > \u001b[32m0\u001b[39m))\n\u001b[32m      3\u001b[39m fig = px.histogram(\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     \u001b[43msalary_df\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtoPandas\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m      5\u001b[39m     x=\u001b[33m\"\u001b[39m\u001b[33mSALARY\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      6\u001b[39m     nbins=\u001b[32m50\u001b[39m,\n\u001b[32m      7\u001b[39m     title=\u001b[33m\"\u001b[39m\u001b[33mSalary Distribution\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      8\u001b[39m )\n\u001b[32m     10\u001b[39m fig.update_layout(bargap=\u001b[32m0.1\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/repo3/.venv/lib/python3.12/site-packages/pyspark/sql/classic/dataframe.py:1792\u001b[39m, in \u001b[36mDataFrame.toPandas\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1791\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtoPandas\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[33m\"\u001b[39m\u001b[33mPandasDataFrameLike\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m1792\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPandasConversionMixin\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtoPandas\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/repo3/.venv/lib/python3.12/site-packages/pyspark/sql/pandas/conversion.py:197\u001b[39m, in \u001b[36mPandasConversionMixin.toPandas\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    196\u001b[39m \u001b[38;5;66;03m# Below is toPandas without Arrow optimization.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m197\u001b[39m rows = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    198\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(rows) > \u001b[32m0\u001b[39m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/repo3/.venv/lib/python3.12/site-packages/pyspark/sql/classic/dataframe.py:443\u001b[39m, in \u001b[36mDataFrame.collect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    442\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m._sc):\n\u001b[32m--> \u001b[39m\u001b[32m443\u001b[39m     sock_info = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcollectToPython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    444\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, BatchedSerializer(CPickleSerializer())))\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/repo3/.venv/lib/python3.12/site-packages/py4j/java_gateway.py:1362\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1361\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1362\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1363\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1365\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/repo3/.venv/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py:282\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    281\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m282\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    283\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/repo3/.venv/lib/python3.12/site-packages/py4j/protocol.py:327\u001b[39m, in \u001b[36mget_return_value\u001b[39m\u001b[34m(answer, gateway_client, target_id, name)\u001b[39m\n\u001b[32m    326\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[32m1\u001b[39m] == REFERENCE_TYPE:\n\u001b[32m--> \u001b[39m\u001b[32m327\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[32m    328\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    329\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name), value)\n\u001b[32m    330\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
            "\u001b[31m<class 'str'>\u001b[39m: (<class 'ConnectionRefusedError'>, ConnectionRefusedError(111, 'Connection refused'))",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[31mConnectionRefusedError\u001b[39m                    Traceback (most recent call last)",
            "    \u001b[31m[... skipping hidden 1 frame]\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/repo3/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py:2184\u001b[39m, in \u001b[36mInteractiveShell.showtraceback\u001b[39m\u001b[34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[39m\n\u001b[32m   2181\u001b[39m         traceback.print_exc()\n\u001b[32m   2182\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2184\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_showtraceback\u001b[49m\u001b[43m(\u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2185\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.call_pdb:\n\u001b[32m   2186\u001b[39m     \u001b[38;5;66;03m# drop into debugger\u001b[39;00m\n\u001b[32m   2187\u001b[39m     \u001b[38;5;28mself\u001b[39m.debugger(force=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/repo3/.venv/lib/python3.12/site-packages/ipykernel/zmqshell.py:559\u001b[39m, in \u001b[36mZMQInteractiveShell._showtraceback\u001b[39m\u001b[34m(self, etype, evalue, stb)\u001b[39m\n\u001b[32m    553\u001b[39m sys.stdout.flush()\n\u001b[32m    554\u001b[39m sys.stderr.flush()\n\u001b[32m    556\u001b[39m exc_content = {\n\u001b[32m    557\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtraceback\u001b[39m\u001b[33m\"\u001b[39m: stb,\n\u001b[32m    558\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mename\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(etype.\u001b[34m__name__\u001b[39m),\n\u001b[32m--> \u001b[39m\u001b[32m559\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mevalue\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mevalue\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m    560\u001b[39m }\n\u001b[32m    562\u001b[39m dh = \u001b[38;5;28mself\u001b[39m.displayhook\n\u001b[32m    563\u001b[39m \u001b[38;5;66;03m# Send exception info over pub socket for other clients than the caller\u001b[39;00m\n\u001b[32m    564\u001b[39m \u001b[38;5;66;03m# to pick up\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/repo3/.venv/lib/python3.12/site-packages/py4j/protocol.py:472\u001b[39m, in \u001b[36mPy4JJavaError.__str__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    470\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__str__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    471\u001b[39m     gateway_client = \u001b[38;5;28mself\u001b[39m.java_exception._gateway_client\n\u001b[32m--> \u001b[39m\u001b[32m472\u001b[39m     answer = \u001b[43mgateway_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mexception_cmd\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    473\u001b[39m     return_value = get_return_value(answer, gateway_client, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    474\u001b[39m     \u001b[38;5;66;03m# Note: technically this should return a bytestring 'str' rather than\u001b[39;00m\n\u001b[32m    475\u001b[39m     \u001b[38;5;66;03m# unicodes in Python 2; however, it can return unicodes for now.\u001b[39;00m\n\u001b[32m    476\u001b[39m     \u001b[38;5;66;03m# See https://github.com/bartdag/py4j/issues/306 for more details.\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/repo3/.venv/lib/python3.12/site-packages/py4j/java_gateway.py:1036\u001b[39m, in \u001b[36mGatewayClient.send_command\u001b[39m\u001b[34m(self, command, retry, binary)\u001b[39m\n\u001b[32m   1015\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msend_command\u001b[39m(\u001b[38;5;28mself\u001b[39m, command, retry=\u001b[38;5;28;01mTrue\u001b[39;00m, binary=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m   1016\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Sends a command to the JVM. This method is not intended to be\u001b[39;00m\n\u001b[32m   1017\u001b[39m \u001b[33;03m       called directly by Py4J users. It is usually called by\u001b[39;00m\n\u001b[32m   1018\u001b[39m \u001b[33;03m       :class:`JavaMember` instances.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1034\u001b[39m \u001b[33;03m     if `binary` is `True`.\u001b[39;00m\n\u001b[32m   1035\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1036\u001b[39m     connection = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1037\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1038\u001b[39m         response = connection.send_command(command)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/repo3/.venv/lib/python3.12/site-packages/py4j/clientserver.py:284\u001b[39m, in \u001b[36mJavaClient._get_connection\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    281\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m    283\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m connection.socket \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m284\u001b[39m     connection = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_create_new_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/repo3/.venv/lib/python3.12/site-packages/py4j/clientserver.py:291\u001b[39m, in \u001b[36mJavaClient._create_new_connection\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    287\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_create_new_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    288\u001b[39m     connection = ClientServerConnection(\n\u001b[32m    289\u001b[39m         \u001b[38;5;28mself\u001b[39m.java_parameters, \u001b[38;5;28mself\u001b[39m.python_parameters,\n\u001b[32m    290\u001b[39m         \u001b[38;5;28mself\u001b[39m.gateway_property, \u001b[38;5;28mself\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m291\u001b[39m     \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect_to_java_server\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    292\u001b[39m     \u001b[38;5;28mself\u001b[39m.set_thread_connection(connection)\n\u001b[32m    293\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/repo3/.venv/lib/python3.12/site-packages/py4j/clientserver.py:438\u001b[39m, in \u001b[36mClientServerConnection.connect_to_java_server\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    435\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ssl_context:\n\u001b[32m    436\u001b[39m     \u001b[38;5;28mself\u001b[39m.socket = \u001b[38;5;28mself\u001b[39m.ssl_context.wrap_socket(\n\u001b[32m    437\u001b[39m         \u001b[38;5;28mself\u001b[39m.socket, server_hostname=\u001b[38;5;28mself\u001b[39m.java_address)\n\u001b[32m--> \u001b[39m\u001b[32m438\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msocket\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mjava_address\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mjava_port\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    439\u001b[39m \u001b[38;5;28mself\u001b[39m.stream = \u001b[38;5;28mself\u001b[39m.socket.makefile(\u001b[33m\"\u001b[39m\u001b[33mrb\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    440\u001b[39m \u001b[38;5;28mself\u001b[39m.is_connected = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
            "\u001b[31mConnectionRefusedError\u001b[39m: [Errno 111] Connection refused"
          ]
        }
      ],
      "source": [
        "# Histogram of SALARY distribution\n",
        "salary_df = df.filter(col(\"SALARY\").isNotNull() & (col(\"SALARY\") > 0))\n",
        "fig = px.histogram(\n",
        "    salary_df.toPandas(),\n",
        "    x=\"SALARY\",\n",
        "    nbins=50,\n",
        "    title=\"Salary Distribution\"\n",
        ")\n",
        "\n",
        "fig.update_layout(bargap=0.1)\n",
        "fig"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d9f11e32-845f-4ed0-aaba-a9a365113499",
      "metadata": {},
      "source": [
        "# 1. Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32c11542",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 1: Casting salary and experience columns\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "df = df.withColumn(\"SALARY\", col(\"SALARY\").cast(\"float\")) \\\n",
        "       .withColumn(\"SALARY_FROM\", col(\"SALARY_FROM\").cast(\"float\")) \\\n",
        "       .withColumn(\"SALARY_TO\", col(\"SALARY_TO\").cast(\"float\")) \\\n",
        "       .withColumn(\"MIN_YEARS_EXPERIENCE\", col(\"MIN_YEARS_EXPERIENCE\").cast(\"float\"))\\\n",
        "       .withColumn(\"MAX_YEARS_EXPERIENCE\", col(\"MAX_YEARS_EXPERIENCE\").cast(\"float\"))\n",
        "\n",
        "# Step 2: Computing medians for salary columns\n",
        "def compute_median(sdf, col_name):\n",
        "    q = sdf.approxQuantile(col_name, [0.5], 0.01)\n",
        "    return q[0] if q else None\n",
        "\n",
        "median_from = compute_median(df, \"SALARY_FROM\")\n",
        "median_to = compute_median(df, \"SALARY_TO\")\n",
        "median_salary = compute_median(df, \"SALARY\")\n",
        "\n",
        "print(\"Medians:\", median_from, median_to)\n",
        "\n",
        "# Step 3: Imputing missing salaries, but not experience\n",
        "df = df.fillna({\n",
        "    \"SALARY_FROM\": median_from,\n",
        "    \"SALARY_TO\": median_to\n",
        "    \"SALARY\": median_salary\n",
        "})\n",
        "\n",
        "# Step 5: Computing average salary\n",
        "df = df.withColumn(\"Average_Salary\",\n",
        "                   (col(\"SALARY_FROM\") + col(\"SALARY_TO\")) / 2)\n",
        "\n",
        "# Step 6: Selecting required columns\n",
        "export_cols = [\n",
        "    \"EDUCATION_LEVELS_NAME\",\n",
        "    \"REMOTE_TYPE_NAME\",\n",
        "    \"MAX_YEARS_EXPERIENCE\",\n",
        "    \"Average_Salary\",\n",
        "    \"SALARY\",\n",
        "    \"LOT_V6_SPECIALIZED_OCCUPATION_NAME\"\n",
        "]\n",
        "df_selected = df.select(*export_cols)\n",
        "\n",
        "# Step 7: Saving to CSV\n",
        "pdf = df_selected.toPandas()\n",
        "pdf.to_csv(\"data/lightcast_cleaned.csv\", index=False)\n",
        "\n",
        "print(\"Data cleaning complete. Rows retained:\", len(pdf))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d5325e67-bb3d-4e5e-984f-042a5329111b",
      "metadata": {},
      "source": [
        "## 1.1 Salary Distribution by Industry and Employment Type\n",
        "\n",
        "-   Compare salary variations across industries.\n",
        "\n",
        "**Filter the dataset** - Remove records where **salary is missing or\n",
        "zero**.\n",
        "\n",
        "**Aggregate Data** - Group by **NAICS industry codes** (e.g.,\n",
        "`NAICS2_NAME`). - Group by **employment type** (`EMPLOYMENT_TYPE_NAME`)\n",
        "and compute salary distribution. - Calculate **salary percentiles**\n",
        "(25th, 50th, 75th) for each group.\n",
        "\n",
        "**Visualize results** - Create a **box plot** where: - **X-axis** =\n",
        "`NAICS2_NAME` - **Y-axis** = `SALARY_FROM`, or `SALARY_TO`, or\n",
        "`SALARY` - **Group/color** = `EMPLOYMENT_TYPE_NAME` - Customize colors,\n",
        "fonts, and styles.\n",
        "\n",
        "**Explanation:** Write two sentences about what the graph reveals (e.g.,\n",
        "median differences across industries and dispersion by employment type)."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv (3.12.3)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
