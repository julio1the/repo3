[
  {
    "objectID": "assignment03-juliovargas.html",
    "href": "assignment03-juliovargas.html",
    "title": "Assignment 03",
    "section": "",
    "text": "from pyspark.sql import SparkSession\nfrom pyspark.sql import functions as F\nfrom pyspark.sql.functions import col, split, explode, regexp_replace, transform, when\nfrom pyspark.sql.functions import col, monotonically_increasing_id\nfrom pyspark.sql.types import StructType  # to/from JSON\n\nimport json\nimport re\nimport numpy as np\nimport pandas as pd\n\nimport plotly.express as px\nimport plotly.io as pio\nimport plotly.graph_objects as go\n\n\nnp.random.seed(30)  # set a fixed seed for reproducibility\npio.renderers.default = \"vscode+notebook\"   #\n# Initialize Spark Session\nspark = SparkSession.builder.appName(\"JobPostingsAnalysis\").getOrCreate()\n# Load schema from JSON file\nwith open(\"data/schema_lightcast.json\") as f:\n    schema = StructType.fromJson(json.load(f))\n\n# Load Data\ndf = (spark.read\n      .option(\"header\", \"true\")\n      .option(\"inferSchema\", \"false\")\n      .schema(schema)              # saved schema\n      .option(\"multiLine\", \"true\")\n      .option(\"escape\", \"\\\"\")\n      .csv(\"data/lightcast_job_postings.csv\")\n      .limit(5000))\n# Show Schema and Sample Data\n#df.printSchema()  \n#df.show(5)\n\nWARNING: Using incubator modules: jdk.incubator.vector\nUsing Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\nSetting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n25/09/22 07:25:08 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n# Histogram of SALARY distribution (cast + filter)\nsalary_df = (\n    df.select(col(\"SALARY\").cast(\"float\"))\n      .filter(col(\"SALARY\").isNotNull() & (col(\"SALARY\") &gt; 0))\n)\n\nfig = px.histogram(\n    salary_df.toPandas(),\n    x=\"SALARY\",\n    nbins=50,\n    title=\"Salary Distribution\"\n)\nfig.update_layout(bargap=0.1)\nfig\n\n[Stage 0:&gt;                                                          (0 + 1) / 1]"
  },
  {
    "objectID": "assignment03_sol.html",
    "href": "assignment03_sol.html",
    "title": "Assignment 03",
    "section": "",
    "text": "Julio Vargas (Boston University)\nSeptember 21, 2025\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql import functions as F\nfrom pyspark.sql.functions import col, split, explode, regexp_replace, transform, when\nfrom pyspark.sql.functions import col, monotonically_increasing_id\nfrom pyspark.sql.types import StructType  # to/from JSON\n\nimport json\nimport re\nimport numpy as np\nimport pandas as pd\n\nimport plotly.express as px\nimport plotly.io as pio\nimport plotly.graph_objects as go\n\n\nnp.random.seed(30)  # set a fixed seed for reproducibility\npio.renderers.default = \"notebook\"\n# Initialize Spark Session\nspark = SparkSession.builder.appName(\"JobPostingsAnalysis\").getOrCreate()\n# Load schema from JSON file\nwith open(\"data/schema_lightcast.json\") as f:\n    schema = StructType.fromJson(json.load(f))\n\n# Load Data\ndf = (spark.read\n      .option(\"header\", \"true\")\n      .option(\"inferSchema\", \"false\")\n      .schema(schema)              # saved schema\n      .option(\"multiLine\", \"true\")\n      .option(\"escape\", \"\\\"\")\n      .csv(\"data/lightcast_job_postings.csv\"))\n# Show Schema and Sample Data\n#df.printSchema()  \n#df.show(5)\n\nWARNING: Using incubator modules: jdk.incubator.vector\nUsing Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\nSetting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n25/09/21 23:31:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n# Histogram of SALARY distribution\nsalary_df = df.filter(col(\"SALARY\").isNotNull() & (col(\"SALARY\") &gt; 0))\nfig = px.histogram(\n    salary_df.toPandas(),\n    x=\"SALARY\",\n    nbins=50,\n    title=\"Salary Distribution\"\n)\n\nfig.update_layout(bargap=0.1)\nfig\n\n25/09/21 23:31:59 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n25/09/21 23:32:12 ERROR Executor: Exception in task 0.0 in stage 0.0 (TID 0)/ 1]\njava.lang.OutOfMemoryError: Java heap space\n    at java.base/java.nio.HeapByteBuffer.&lt;init&gt;(HeapByteBuffer.java:64)\n    at java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:363)\n    at org.apache.spark.serializer.SerializerHelper$.$anonfun$serializeToChunkedBuffer$1(SerializerHelper.scala:40)\n    at org.apache.spark.serializer.SerializerHelper$.$anonfun$serializeToChunkedBuffer$1$adapted(SerializerHelper.scala:40)\n    at org.apache.spark.serializer.SerializerHelper$$$Lambda$3196/0x00007240ccf204b8.apply(Unknown Source)\n    at org.apache.spark.util.io.ChunkedByteBufferOutputStream.allocateNewChunkIfNeeded(ChunkedByteBufferOutputStream.scala:87)\n    at org.apache.spark.util.io.ChunkedByteBufferOutputStream.write(ChunkedByteBufferOutputStream.scala:75)\n    at java.base/java.io.ObjectOutputStream$BlockDataOutputStream.write(ObjectOutputStream.java:1862)\n    at java.base/java.io.ObjectOutputStream.write(ObjectOutputStream.java:714)\n    at org.apache.spark.util.Utils$.$anonfun$writeByteBuffer$1(Utils.scala:217)\n    at org.apache.spark.util.Utils$.$anonfun$writeByteBuffer$1$adapted(Utils.scala:217)\n    at org.apache.spark.util.Utils$$$Lambda$3199/0x00007240ccf21fe0.apply(Unknown Source)\n    at org.apache.spark.util.Utils$.writeByteBufferImpl(Utils.scala:195)\n    at org.apache.spark.util.Utils$.writeByteBuffer(Utils.scala:217)\n    at org.apache.spark.util.io.ChunkedByteBuffer.$anonfun$writeExternal$2(ChunkedByteBuffer.scala:104)\n    at org.apache.spark.util.io.ChunkedByteBuffer.$anonfun$writeExternal$2$adapted(ChunkedByteBuffer.scala:104)\n    at org.apache.spark.util.io.ChunkedByteBuffer$$Lambda$3198/0x00007240ccf21c10.apply(Unknown Source)\n    at scala.collection.ArrayOps$.foreach$extension(ArrayOps.scala:1324)\n    at org.apache.spark.util.io.ChunkedByteBuffer.writeExternal(ChunkedByteBuffer.scala:104)\n    at org.apache.spark.scheduler.DirectTaskResult.$anonfun$writeExternal$1(TaskResult.scala:60)\n    at org.apache.spark.scheduler.DirectTaskResult$$Lambda$3205/0x00007240ccf238a0.apply$mcV$sp(Unknown Source)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n    at org.apache.spark.util.SparkErrorUtils.tryOrIOException(SparkErrorUtils.scala:35)\n    at org.apache.spark.util.SparkErrorUtils.tryOrIOException$(SparkErrorUtils.scala:33)\n    at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:99)\n    at org.apache.spark.scheduler.DirectTaskResult.writeExternal(TaskResult.scala:59)\n    at java.base/java.io.ObjectOutputStream.writeExternalData(ObjectOutputStream.java:1465)\n    at java.base/java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1436)\n    at java.base/java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1181)\n    at java.base/java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:350)\n    at org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:47)\n    at org.apache.spark.serializer.SerializerHelper$.serializeToChunkedBuffer(SerializerHelper.scala:42)\n25/09/21 23:32:12 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 0.0 in stage 0.0 (TID 0),5,main]\njava.lang.OutOfMemoryError: Java heap space\n    at java.base/java.nio.HeapByteBuffer.&lt;init&gt;(HeapByteBuffer.java:64)\n    at java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:363)\n    at org.apache.spark.serializer.SerializerHelper$.$anonfun$serializeToChunkedBuffer$1(SerializerHelper.scala:40)\n    at org.apache.spark.serializer.SerializerHelper$.$anonfun$serializeToChunkedBuffer$1$adapted(SerializerHelper.scala:40)\n    at org.apache.spark.serializer.SerializerHelper$$$Lambda$3196/0x00007240ccf204b8.apply(Unknown Source)\n    at org.apache.spark.util.io.ChunkedByteBufferOutputStream.allocateNewChunkIfNeeded(ChunkedByteBufferOutputStream.scala:87)\n    at org.apache.spark.util.io.ChunkedByteBufferOutputStream.write(ChunkedByteBufferOutputStream.scala:75)\n    at java.base/java.io.ObjectOutputStream$BlockDataOutputStream.write(ObjectOutputStream.java:1862)\n    at java.base/java.io.ObjectOutputStream.write(ObjectOutputStream.java:714)\n    at org.apache.spark.util.Utils$.$anonfun$writeByteBuffer$1(Utils.scala:217)\n    at org.apache.spark.util.Utils$.$anonfun$writeByteBuffer$1$adapted(Utils.scala:217)\n    at org.apache.spark.util.Utils$$$Lambda$3199/0x00007240ccf21fe0.apply(Unknown Source)\n    at org.apache.spark.util.Utils$.writeByteBufferImpl(Utils.scala:195)\n    at org.apache.spark.util.Utils$.writeByteBuffer(Utils.scala:217)\n    at org.apache.spark.util.io.ChunkedByteBuffer.$anonfun$writeExternal$2(ChunkedByteBuffer.scala:104)\n    at org.apache.spark.util.io.ChunkedByteBuffer.$anonfun$writeExternal$2$adapted(ChunkedByteBuffer.scala:104)\n    at org.apache.spark.util.io.ChunkedByteBuffer$$Lambda$3198/0x00007240ccf21c10.apply(Unknown Source)\n    at scala.collection.ArrayOps$.foreach$extension(ArrayOps.scala:1324)\n    at org.apache.spark.util.io.ChunkedByteBuffer.writeExternal(ChunkedByteBuffer.scala:104)\n    at org.apache.spark.scheduler.DirectTaskResult.$anonfun$writeExternal$1(TaskResult.scala:60)\n    at org.apache.spark.scheduler.DirectTaskResult$$Lambda$3205/0x00007240ccf238a0.apply$mcV$sp(Unknown Source)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n    at org.apache.spark.util.SparkErrorUtils.tryOrIOException(SparkErrorUtils.scala:35)\n    at org.apache.spark.util.SparkErrorUtils.tryOrIOException$(SparkErrorUtils.scala:33)\n    at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:99)\n    at org.apache.spark.scheduler.DirectTaskResult.writeExternal(TaskResult.scala:59)\n    at java.base/java.io.ObjectOutputStream.writeExternalData(ObjectOutputStream.java:1465)\n    at java.base/java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1436)\n    at java.base/java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1181)\n    at java.base/java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:350)\n    at org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:47)\n    at org.apache.spark.serializer.SerializerHelper$.serializeToChunkedBuffer(SerializerHelper.scala:42)\n25/09/21 23:32:12 WARN TaskSetManager: Lost task 0.0 in stage 0.0 (TID 0) (ip-172-31-28-19.ec2.internal executor driver): java.lang.OutOfMemoryError: Java heap space\n    at java.base/java.nio.HeapByteBuffer.&lt;init&gt;(HeapByteBuffer.java:64)\n    at java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:363)\n    at org.apache.spark.serializer.SerializerHelper$.$anonfun$serializeToChunkedBuffer$1(SerializerHelper.scala:40)\n    at org.apache.spark.serializer.SerializerHelper$.$anonfun$serializeToChunkedBuffer$1$adapted(SerializerHelper.scala:40)\n    at org.apache.spark.serializer.SerializerHelper$$$Lambda$3196/0x00007240ccf204b8.apply(Unknown Source)\n    at org.apache.spark.util.io.ChunkedByteBufferOutputStream.allocateNewChunkIfNeeded(ChunkedByteBufferOutputStream.scala:87)\n    at org.apache.spark.util.io.ChunkedByteBufferOutputStream.write(ChunkedByteBufferOutputStream.scala:75)\n    at java.base/java.io.ObjectOutputStream$BlockDataOutputStream.write(ObjectOutputStream.java:1862)\n    at java.base/java.io.ObjectOutputStream.write(ObjectOutputStream.java:714)\n    at org.apache.spark.util.Utils$.$anonfun$writeByteBuffer$1(Utils.scala:217)\n    at org.apache.spark.util.Utils$.$anonfun$writeByteBuffer$1$adapted(Utils.scala:217)\n    at org.apache.spark.util.Utils$$$Lambda$3199/0x00007240ccf21fe0.apply(Unknown Source)\n    at org.apache.spark.util.Utils$.writeByteBufferImpl(Utils.scala:195)\n    at org.apache.spark.util.Utils$.writeByteBuffer(Utils.scala:217)\n    at org.apache.spark.util.io.ChunkedByteBuffer.$anonfun$writeExternal$2(ChunkedByteBuffer.scala:104)\n    at org.apache.spark.util.io.ChunkedByteBuffer.$anonfun$writeExternal$2$adapted(ChunkedByteBuffer.scala:104)\n    at org.apache.spark.util.io.ChunkedByteBuffer$$Lambda$3198/0x00007240ccf21c10.apply(Unknown Source)\n    at scala.collection.ArrayOps$.foreach$extension(ArrayOps.scala:1324)\n    at org.apache.spark.util.io.ChunkedByteBuffer.writeExternal(ChunkedByteBuffer.scala:104)\n    at org.apache.spark.scheduler.DirectTaskResult.$anonfun$writeExternal$1(TaskResult.scala:60)\n    at org.apache.spark.scheduler.DirectTaskResult$$Lambda$3205/0x00007240ccf238a0.apply$mcV$sp(Unknown Source)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n    at org.apache.spark.util.SparkErrorUtils.tryOrIOException(SparkErrorUtils.scala:35)\n    at org.apache.spark.util.SparkErrorUtils.tryOrIOException$(SparkErrorUtils.scala:33)\n    at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:99)\n    at org.apache.spark.scheduler.DirectTaskResult.writeExternal(TaskResult.scala:59)\n    at java.base/java.io.ObjectOutputStream.writeExternalData(ObjectOutputStream.java:1465)\n    at java.base/java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1436)\n    at java.base/java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1181)\n    at java.base/java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:350)\n    at org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:47)\n    at org.apache.spark.serializer.SerializerHelper$.serializeToChunkedBuffer(SerializerHelper.scala:42)\n\n25/09/21 23:32:12 ERROR TaskSetManager: Task 0 in stage 0.0 failed 1 times; aborting job\n[Stage 0:&gt;                                                          (0 + 0) / 1]\n\n\n\n---------------------------------------------------------------------------\nPy4JJavaError                             Traceback (most recent call last)\n    [... skipping hidden 1 frame]\n\nCell In[2], line 4\n      2 salary_df = df.filter(col(\"SALARY\").isNotNull() & (col(\"SALARY\") &gt; 0))\n      3 fig = px.histogram(\n----&gt; 4     salary_df.toPandas(),\n      5     x=\"SALARY\",\n      6     nbins=50,\n      7     title=\"Salary Distribution\"\n      8 )\n     10 fig.update_layout(bargap=0.1)\n\nFile ~/repo3/.venv/lib/python3.12/site-packages/pyspark/sql/classic/dataframe.py:1792, in DataFrame.toPandas(self)\n   1791 def toPandas(self) -&gt; \"PandasDataFrameLike\":\n-&gt; 1792     return PandasConversionMixin.toPandas(self)\n\nFile ~/repo3/.venv/lib/python3.12/site-packages/pyspark/sql/pandas/conversion.py:197, in PandasConversionMixin.toPandas(self)\n    196 # Below is toPandas without Arrow optimization.\n--&gt; 197 rows = self.collect()\n    198 if len(rows) &gt; 0:\n\nFile ~/repo3/.venv/lib/python3.12/site-packages/pyspark/sql/classic/dataframe.py:443, in DataFrame.collect(self)\n    442 with SCCallSiteSync(self._sc):\n--&gt; 443     sock_info = self._jdf.collectToPython()\n    444 return list(_load_from_socket(sock_info, BatchedSerializer(CPickleSerializer())))\n\nFile ~/repo3/.venv/lib/python3.12/site-packages/py4j/java_gateway.py:1362, in JavaMember.__call__(self, *args)\n   1361 answer = self.gateway_client.send_command(command)\n-&gt; 1362 return_value = get_return_value(\n   1363     answer, self.gateway_client, self.target_id, self.name)\n   1365 for temp_arg in temp_args:\n\nFile ~/repo3/.venv/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py:282, in capture_sql_exception.&lt;locals&gt;.deco(*a, **kw)\n    281 try:\n--&gt; 282     return f(*a, **kw)\n    283 except Py4JJavaError as e:\n\nFile ~/repo3/.venv/lib/python3.12/site-packages/py4j/protocol.py:327, in get_return_value(answer, gateway_client, target_id, name)\n    326 if answer[1] == REFERENCE_TYPE:\n--&gt; 327     raise Py4JJavaError(\n    328         \"An error occurred while calling {0}{1}{2}.\\n\".\n    329         format(target_id, \".\", name), value)\n    330 else:\n\n&lt;class 'str'&gt;: (&lt;class 'ConnectionRefusedError'&gt;, ConnectionRefusedError(111, 'Connection refused'))\n\nDuring handling of the above exception, another exception occurred:\n\nConnectionRefusedError                    Traceback (most recent call last)\n    [... skipping hidden 1 frame]\n\nFile ~/repo3/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py:2184, in InteractiveShell.showtraceback(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\n   2181         traceback.print_exc()\n   2182         return None\n-&gt; 2184     self._showtraceback(etype, value, stb)\n   2185 if self.call_pdb:\n   2186     # drop into debugger\n   2187     self.debugger(force=True)\n\nFile ~/repo3/.venv/lib/python3.12/site-packages/ipykernel/zmqshell.py:559, in ZMQInteractiveShell._showtraceback(self, etype, evalue, stb)\n    553 sys.stdout.flush()\n    554 sys.stderr.flush()\n    556 exc_content = {\n    557     \"traceback\": stb,\n    558     \"ename\": str(etype.__name__),\n--&gt; 559     \"evalue\": str(evalue),\n    560 }\n    562 dh = self.displayhook\n    563 # Send exception info over pub socket for other clients than the caller\n    564 # to pick up\n\nFile ~/repo3/.venv/lib/python3.12/site-packages/py4j/protocol.py:472, in Py4JJavaError.__str__(self)\n    470 def __str__(self):\n    471     gateway_client = self.java_exception._gateway_client\n--&gt; 472     answer = gateway_client.send_command(self.exception_cmd)\n    473     return_value = get_return_value(answer, gateway_client, None, None)\n    474     # Note: technically this should return a bytestring 'str' rather than\n    475     # unicodes in Python 2; however, it can return unicodes for now.\n    476     # See https://github.com/bartdag/py4j/issues/306 for more details.\n\nFile ~/repo3/.venv/lib/python3.12/site-packages/py4j/java_gateway.py:1036, in GatewayClient.send_command(self, command, retry, binary)\n   1015 def send_command(self, command, retry=True, binary=False):\n   1016     \"\"\"Sends a command to the JVM. This method is not intended to be\n   1017        called directly by Py4J users. It is usually called by\n   1018        :class:`JavaMember` instances.\n   (...)   1034      if `binary` is `True`.\n   1035     \"\"\"\n-&gt; 1036     connection = self._get_connection()\n   1037     try:\n   1038         response = connection.send_command(command)\n\nFile ~/repo3/.venv/lib/python3.12/site-packages/py4j/clientserver.py:284, in JavaClient._get_connection(self)\n    281     pass\n    283 if connection is None or connection.socket is None:\n--&gt; 284     connection = self._create_new_connection()\n    285 return connection\n\nFile ~/repo3/.venv/lib/python3.12/site-packages/py4j/clientserver.py:291, in JavaClient._create_new_connection(self)\n    287 def _create_new_connection(self):\n    288     connection = ClientServerConnection(\n    289         self.java_parameters, self.python_parameters,\n    290         self.gateway_property, self)\n--&gt; 291     connection.connect_to_java_server()\n    292     self.set_thread_connection(connection)\n    293     return connection\n\nFile ~/repo3/.venv/lib/python3.12/site-packages/py4j/clientserver.py:438, in ClientServerConnection.connect_to_java_server(self)\n    435 if self.ssl_context:\n    436     self.socket = self.ssl_context.wrap_socket(\n    437         self.socket, server_hostname=self.java_address)\n--&gt; 438 self.socket.connect((self.java_address, self.java_port))\n    439 self.stream = self.socket.makefile(\"rb\")\n    440 self.is_connected = True\n\nConnectionRefusedError: [Errno 111] Connection refused"
  },
  {
    "objectID": "assignment03_sol.html#salary-distribution-by-industry-and-employment-type",
    "href": "assignment03_sol.html#salary-distribution-by-industry-and-employment-type",
    "title": "Assignment 03",
    "section": "1.1 Salary Distribution by Industry and Employment Type",
    "text": "1.1 Salary Distribution by Industry and Employment Type\n\nCompare salary variations across industries.\n\nFilter the dataset - Remove records where salary is missing or zero.\nAggregate Data - Group by NAICS industry codes (e.g., NAICS2_NAME). - Group by employment type (EMPLOYMENT_TYPE_NAME) and compute salary distribution. - Calculate salary percentiles (25th, 50th, 75th) for each group.\nVisualize results - Create a box plot where: - X-axis = NAICS2_NAME - Y-axis = SALARY_FROM, or SALARY_TO, or SALARY - Group/color = EMPLOYMENT_TYPE_NAME - Customize colors, fonts, and styles.\nExplanation: Write two sentences about what the graph reveals (e.g., median differences across industries and dispersion by employment type)."
  },
  {
    "objectID": "assignment03-juliovargas.html#salary-distribution-by-industry-and-employment-type",
    "href": "assignment03-juliovargas.html#salary-distribution-by-industry-and-employment-type",
    "title": "Assignment 03",
    "section": "1.1 Salary Distribution by Industry and Employment Type",
    "text": "1.1 Salary Distribution by Industry and Employment Type\n\nCompare salary variations across industries.\n\nFilter the dataset - Remove records where salary is missing or zero.\nAggregate Data - Group by NAICS industry codes (e.g., NAICS2_NAME). - Group by employment type (EMPLOYMENT_TYPE_NAME) and compute salary distribution. - Calculate salary percentiles (25th, 50th, 75th) for each group.\nVisualize results - Create a box plot where: - X-axis = NAICS2_NAME - Y-axis = SALARY_FROM, or SALARY_TO, or SALARY - Group/color = EMPLOYMENT_TYPE_NAME - Customize colors, fonts, and styles.\nExplanation: Write two sentences about what the graph reveals (e.g., median differences across industries and dispersion by employment type).\n\n#your code for first query\nimport pandas as pd\nimport polars as pl\nfrom IPython.display import display, HTML\n\n# Filter out missing or zero salary values\npdf = df.filter(df[\"SALARY\"] &gt; 0).select(\"EMPLOYMENT_TYPE_NAME\", \"SALARY\").toPandas()\n\n\n# Clean employment type names for better readability\npdf[\"EMPLOYMENT_TYPE_NAME\"] = (\n    pdf[\"EMPLOYMENT_TYPE_NAME\"]\n      .astype(str)\n      .str.replace(r\"[^\\x00-\\x7F]+\", \"\", regex=True)\n)\n\n#display(HTML(f\"&lt;div style='height:300px; overflow:auto'&gt;{pdf.iloc[:10].to_html(index=False)}&lt;/div&gt;\"))\n\n# Compute median salary for sorting\nmedian_salaries = pdf.groupby(\"EMPLOYMENT_TYPE_NAME\")[\"SALARY\"].median()\ndisplay(median_salaries.to_frame().head())\n\n\n# Sort employment types based on median salary in descending order\nsorted_employment_types = median_salaries.sort_values(ascending=False).index\n\n# Apply sorted categories\npdf[\"EMPLOYMENT_TYPE_NAME\"] = pd.Categorical(\n    pdf[\"EMPLOYMENT_TYPE_NAME\"],\n    categories=sorted_employment_types,\n    ordered=True\n)\n\n# Create box plot with horizontal grid lines\nfig = px.box(\n    pdf,\n    x=\"EMPLOYMENT_TYPE_NAME\",\n    y=\"SALARY\",\n    title=\"Salary Distribution by Employment Type\",\n    color_discrete_sequence=[\"#CC0000\"],  # Single neutral color\n    boxmode=\"group\",\n    points=\"all\"  # Show all outliers\n)\nfig\n\n\n\n# Improve layout, font styles, and axis labels\nfig.update_layout(\n    title=dict(\n        text=\"Salary Distribution by Employment Type\",\n        font=dict(size=16, family=\"Helvetica\", color=\"black\")  # Bigger & Bold Title\n    ),\n    xaxis=dict(\n        title=dict(text=\"Employment Type\", font=dict(size=14, family=\"Helvetica\", color=\"black\",weight=\"bold\")),  # Bigger X-label\n        tickangle=0,  # Rotate X-axis labels for readability\n        tickfont=dict(size=12, family=\"Helvetica\", color=\"black\"),  # Bigger & Bold X-ticks\n        showline=True,  # Show axis lines\n        linewidth=2,    # Thicker axis lines\n        linecolor=\"black\",\n        mirror=True,\n        showgrid=False,  # Remove vertical grid lines\n        categoryorder=\"array\",\n        categoryarray=sorted_employment_types.tolist()\n    ),\n    yaxis=dict(\n        title=dict(text=\"Salary (in $1000)\", font=dict(size=14, family=\"Helvetica\", color=\"black\",weight=\"bold\")),  # Bigger Y-label\n        tickvals=[0, 50000, 100000, 150000, 200000, 250000, 300000, 350000, 400000, 450000, 500000],\n        ticktext=[\"0\", \"50\", \"100\", \"150\", \"200\", \"250\", \"300\", \"350\", \"400\", \"450\", \"500\"],\n        tickfont=dict(size=12, family=\"Helvetica\", color=\"black\"),  # Bigger & Bold Y-ticks\n        showline=True,\n        linewidth=2,\n        linecolor=\"black\",\n        mirror=True,\n        showgrid=True,       # Enable light horizontal grid lines\n        gridcolor=\"lightgray\",  # Light shade for the horizontal grid\n        gridwidth=0.5        # Thin grid lines\n    ),\n    font=dict(family=\"Helvetica\", size=12, color=\"black\"),\n    boxgap=0.7,\n    plot_bgcolor=\"white\",\n    paper_bgcolor=\"white\",\n    showlegend=False,\n    height=500,\n    width=850\n)\n\n# Show & export\nfig.show()\nfig.write_html(\"output/Q1.html\")\nfig.write_image(\"output/Q1.svg\", width=850, height=500, scale=1)\n\n\n\n\n\n\n\n\nSALARY\n\n\nEMPLOYMENT_TYPE_NAME\n\n\n\n\n\nFull-time (&gt; 32 hours)\n116490.0\n\n\nNone\n116490.0\n\n\nPart-time ( 32 hours)\n116490.0\n\n\nPart-time / full-time\n116490.0"
  }
]